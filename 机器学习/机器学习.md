# 机器学习

- [机器学习](#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0)
  - [I. 模型评估与选择](#i-%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e4%b8%8e%e9%80%89%e6%8b%a9)
    - [1.1 误差与拟合](#11-%e8%af%af%e5%b7%ae%e4%b8%8e%e6%8b%9f%e5%90%88)
    - [1.2 评估方法](#12-%e8%af%84%e4%bc%b0%e6%96%b9%e6%b3%95)
    - [1.3 性能度量 performance measure](#13-%e6%80%a7%e8%83%bd%e5%ba%a6%e9%87%8f-performance-measure)


---

## I. 模型评估与选择

### 1.1 误差与拟合

- **「错误率 error rate」**-- 如果在 $m$ 个样本中有 $a$ 个样本分类错误，则错误率 $E = a/m$。
- **「精度 accuracy」**-- 精度 = 1 - 错误率。
- **「误差 error」**-- 预测输出与样本真实输出之间的差异。
- **「训练误差 training error / 经验误差 empirical error」**-- 学习器在训练集上的误差。
- **「泛化误差 generalization error」**-- 在新样本上的误差。
- **「过拟合 overfitting」**-- 把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质。
- **「欠拟合 underfitting」**-- 与「过拟合」相对。

### 1.2 评估方法

- **「留出法 hold-out」**-- 直接将数据集 $D$ 划分为两个互斥的集合，其中一个集合作为训练集 $S$，另一个作为测试集 $T$。
- **「交叉验证法 cross validation」**-- 先将数据集 $D$ 划分为 $k$ 个大小相似的互斥子集。每个子集 $D_i$ 都尽可能保持数据分布的一致性。每次用 $k - 1$ 个子集的并集作为训练集，余下的那个子集作为测试集；这样就可以获得 $k$ 组训练/测试集，进行 $k$ 次训练和测试，返回这 $k$ 个测试结果的均值。
- **「k 折交叉验证 k-fold cross validation」**-- 交叉验证法评估结果的稳定性和保真性很大程度上取决于 $k$ 值，$k$ 通常取 $10$，此时称为 10 折交叉验证。
- **「留一法 Leave-One-Out, LOO」**-- 假定数据集 $D$ 中包含 $m$ 个样本，若令 $k = m$，则为留一法。留一法评估结果比较准确，但计算开销较大。
- **「自助法 bootstrapping」**-- 给定包含 $m$ 个样本的数据集 $D$，每次随机从 $D$ 中挑选一个样本，将其拷贝放入 $D'$，然后再将该样本放回初始数据集 $D$ 中，使得该样本在下次采样仍有可能被采到。重复 $m$ 次后，得到包含 $m$ 个样本的数据集 $D'$。以上是「自助采样 bootstrap samping」的结果。

### 1.3 性能度量 performance measure

- **「均方误差 mean squared error」**

回归任务常用：

$$E(f;D) = \frac{1}{m} \sum_{i=1}^{m}(f(x_i) - y_i)^2$$

更一般的，对于数据分布 $\mathcal{D}$ 和概率密度函数 $p(·)$，均方误差可描述为

$$E(f;\mathcal{D}) = \int_{x～\mathcal{D}}(f(x) - y)^2p(x)\mathrm{d}x$$

- **「错误率与精度」**

错误率定义为

$$E(f;D) = \frac{1}{m} \sum_{i=1}^m \mathbb{I}(f(x_i) \neq y_i)$$

精度定义为

$$acc(f;D) = \frac{1}{m} \sum_{i=1}^m \mathbb{I} (f(x_i) = y_i)$$ $$= 1 - E(f;D)$$

更一般的，对于数据分布 $\mathcal{D}$ 和概率密度函数 $p(·)$，错误率和精度可分别描述为

$$E(f;\mathcal{D}) = \int_{x～\mathcal{D}} \mathbb{I}(f(x) \neq y)p(x)\mathrm{d}x$$

$$acc(f;\mathcal{D}) = \int_{x～\mathcal{D}}\mathbb{I}(f(x) = y)p(x)\mathrm{d}x$$ $$= 1 - E(f;\mathcal{D})$$

- **「查准率、查全率与 $F1$」**

根据真实类别与学习器预测类别划分： 

**真正例 true positive, TP** / **假正例 false positive, FP** /
**真反例 true negative, TN** / **假反例 false negative, FN**

| 真实 \ 预测 | 正例 | 反例 |
| ----------- | ---- | ---- |
| 正例        | TP   | FN   |
| 反例        | FP   | TN   |

查准率 $$P = \frac{TP}{TP + FP}$$

查全率 $$R = \frac{TP}{TP + FN}$$

平衡点度量 ( Break-Even Point, BEP ) ：当“查准率 = 查全率”时的取值。

F1 度量：

$$F1 = \frac{2 \times P \times R}{P + R} = \frac{2 \times TP}{样例总数 + TP - TN}$$

表达对查准率/查全率不同偏好的 $F_\beta$：

$$F_\beta = \frac{(1 + \beta^2) \times P \times R}{(\beta^2 \times P) + R}$$

$\beta > 0$ 度量了查全率和查准率的相对重要性，$\beta = 1$ 等同于 $F1$，$\beta > 1$ 时查全率有更大影响，$\beta < 1$ 时查准率有更大影响。

> *PS:*
> 
> $F1$ 是基于查准率与查全率的调和平均定义：
> $$\frac{1}{F1} = \frac{1}{2} \cdot \left(\frac{1}{P} + \frac{1}{R}\right)$$
> $F_\beta$ 是加权调和平均：
> $$\frac{1}{F_\beta} = \frac{1}{1 + \beta^2} \cdot \left(\frac{1}{P} + \frac{\beta^2}{R} \right)$$
> 与算术平均 ($\frac{P + R}{2}$) 和几何平均 ($\sqrt{P \times R}$) 相比，调和平均更重视较小值。

- **「ROC 与 AUC」**

ROC 曲线的纵轴是“真正例率”(True Positive Rate, TPR)，横轴是“假正例率”(False Positive Rate, FPR):

$$TPR = \frac{TP}{TP + FN}$$

$$FPR = \frac{FP}{TN + FP}$$

AUC (Area Under ROC Curve) 是 ROC 曲线下的面积。








